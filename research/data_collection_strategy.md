# æ•¸æ“šæ”¶é›†å’Œæ›´æ–°ç­–ç•¥è¨­è¨ˆ

**è¨­è¨ˆæ—¥æœŸ**: 2024-12-12  
**ç›®æ¨™**: è¨­è¨ˆå®Œæ•´çš„æ•¸æ“šæ”¶é›†å’Œæ›´æ–°ç­–ç•¥ï¼Œè§£æ±ºæ•¸æ“šæ™‚æ•ˆæ€§ã€åœ°å€è¦†è“‹ç‡å’Œæ•¸æ“šå®Œæ•´æ€§å•é¡Œ

---

## ğŸ¯ è¨­è¨ˆç›®æ¨™

### æ ¸å¿ƒç›®æ¨™
1. **æ•¸æ“šæ™‚æ•ˆæ€§**: å°‡æ•¸æ“šå»¶é²å¾æ•¸å°æ™‚é™ä½åˆ° **5 åˆ†é˜ä»¥å…§**
2. **åœ°å€è¦†è“‹ç‡**: è¿½è¹¤**æ‰€æœ‰**åƒèˆ‡äº¤æ˜“çš„åœ°å€ï¼ˆå¾æ•¸ç™¾æå‡åˆ°æ•¸è¬ï¼‰
3. **æ•¸æ“šå®Œæ•´æ€§**: æ•ç²**æ‰€æœ‰**è¨‚å–®å¡«å……äº‹ä»¶ï¼Œç„¡éºæ¼
4. **ç³»çµ±ç©©å®šæ€§**: è‡ªå‹•åŒ–é‹è¡Œï¼ŒéŒ¯èª¤è‡ªå‹•æ¢å¾©
5. **å¯æ“´å±•æ€§**: æ”¯æŒæœªä¾†åŠŸèƒ½æ“´å±•

### éåŠŸèƒ½æ€§ç›®æ¨™
- **å¯é æ€§**: 99.9% æ­£å¸¸é‹è¡Œæ™‚é–“
- **æ•ˆèƒ½**: æ¯åˆ†é˜è™•ç† 1000+ äº‹ä»¶
- **å¯ç¶­è­·æ€§**: ä»£ç¢¼æ¸…æ™°ï¼Œæ˜“æ–¼èª¿è©¦
- **æˆæœ¬æ•ˆç›Š**: æœ€å°åŒ– API èª¿ç”¨æ¬¡æ•¸

---

## ğŸ“Š æ•´é«”æ¶æ§‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     æ•¸æ“šæ”¶é›†å’Œæ›´æ–°ç³»çµ±                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. æ•¸æ“šæºå±¤ (Data Sources)                                 â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  â€¢ Goldsky Orderbook Subgraph (æ ¸å¿ƒ)                        â”‚  â”‚
â”‚  â”‚  â€¢ Goldsky PNL Subgraph (è£œå……)                              â”‚  â”‚
â”‚  â”‚  â€¢ Goldsky Activity Subgraph (è£œå……)                         â”‚  â”‚
â”‚  â”‚  â€¢ Polymarket REST API (å¸‚å ´å…ƒæ•¸æ“š)                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  2. æ•¸æ“šæ”¶é›†å±¤ (Data Collection)                            â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  â€¢ OrderbookCollector (ä¸»è¦)                                â”‚  â”‚
â”‚  â”‚    - å¢é‡æ”¶é›† orderFilledEvents                             â”‚  â”‚
â”‚  â”‚    - è‡ªå‹•æ¢å¾©æ©Ÿåˆ¶                                            â”‚  â”‚
â”‚  â”‚    - éŒ¯èª¤è™•ç†å’Œé‡è©¦                                          â”‚  â”‚
â”‚  â”‚  â€¢ MarketCollector (è¼”åŠ©)                                   â”‚  â”‚
â”‚  â”‚    - æ”¶é›†å¸‚å ´å…ƒæ•¸æ“š                                          â”‚  â”‚
â”‚  â”‚    - æ›´æ–°å¸‚å ´ç‹€æ…‹                                            â”‚  â”‚
â”‚  â”‚  â€¢ AddressCollector (è¼”åŠ©)                                  â”‚  â”‚
â”‚  â”‚    - è£œå……åœ°å€æŒå€‰æ•¸æ“š                                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  3. æ•¸æ“šè™•ç†å±¤ (Data Processing)                            â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  â€¢ EventProcessor                                           â”‚  â”‚
â”‚  â”‚    - è§£æ orderFilledEvents                                 â”‚  â”‚
â”‚  â”‚    - æå– maker/taker åœ°å€                                  â”‚  â”‚
â”‚  â”‚    - è¨ˆç®—åƒ¹æ ¼å’Œæ–¹å‘                                          â”‚  â”‚
â”‚  â”‚    - æ¨™æº–åŒ–é‡‘é¡                                              â”‚  â”‚
â”‚  â”‚  â€¢ AddressProcessor                                         â”‚  â”‚
â”‚  â”‚    - æ›´æ–°åœ°å€çµ±è¨ˆ                                            â”‚  â”‚
â”‚  â”‚    - è¨ˆç®—äº¤æ˜“æŒ‡æ¨™                                            â”‚  â”‚
â”‚  â”‚    - è­˜åˆ¥å·¨é¯¨å’Œå¯ç–‘åœ°å€                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  4. æ•¸æ“šå­˜å„²å±¤ (Data Storage)                               â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  â€¢ MySQL æ•¸æ“šåº« (Drizzle ORM)                               â”‚  â”‚
â”‚  â”‚    - trades (äº¤æ˜“è¨˜éŒ„)                                       â”‚  â”‚
â”‚  â”‚    - addresses (åœ°å€æ•¸æ“š)                                    â”‚  â”‚
â”‚  â”‚    - markets (å¸‚å ´æ•¸æ“š)                                      â”‚  â”‚
â”‚  â”‚    - sync_state (åŒæ­¥ç‹€æ…‹)                                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  5. èª¿åº¦å±¤ (Scheduling)                                     â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  â€¢ å¯¦æ™‚æ›´æ–°ä»»å‹™ (æ¯ 5 åˆ†é˜)                                  â”‚  â”‚
â”‚  â”‚    - æ”¶é›†æ–°çš„ orderFilledEvents                             â”‚  â”‚
â”‚  â”‚    - æ›´æ–°åœ°å€çµ±è¨ˆ                                            â”‚  â”‚
â”‚  â”‚  â€¢ æ­·å²è£œå…¨ä»»å‹™ (ä¸€æ¬¡æ€§)                                     â”‚  â”‚
â”‚  â”‚    - å›å¡«æ­·å²äº¤æ˜“æ•¸æ“š                                         â”‚  â”‚
â”‚  â”‚  â€¢ å¸‚å ´æ›´æ–°ä»»å‹™ (æ¯ 1 å°æ™‚)                                  â”‚  â”‚
â”‚  â”‚    - æ›´æ–°å¸‚å ´å…ƒæ•¸æ“š                                          â”‚  â”‚
â”‚  â”‚    - æ›´æ–°åƒ¹æ ¼æ•¸æ“š                                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ å¯¦æ™‚æ•¸æ“šæ›´æ–°æ©Ÿåˆ¶

### æ ¸å¿ƒè¨­è¨ˆï¼šå¢é‡æ›´æ–°

#### åŸç†
- è¨˜éŒ„ä¸Šæ¬¡åŒæ­¥çš„æ™‚é–“æˆ³
- æ¯æ¬¡åªæŸ¥è©¢æ–°çš„äº‹ä»¶ï¼ˆ`timestamp_gt: last_timestamp`ï¼‰
- é¿å…é‡è¤‡è™•ç†æ•¸æ“š

#### å¯¦ç¾

```python
class OrderbookCollector:
    """Orderbook æ•¸æ“šæ”¶é›†å™¨"""
    
    def __init__(self, db_connection):
        self.db = db_connection
        self.subgraph_url = "https://api.goldsky.com/api/public/project_cl6mb8i9h0003e201j6li0diw/subgraphs/orderbook-subgraph/0.0.1/gn"
        self.batch_size = 1000
    
    def get_last_synced_timestamp(self):
        """å¾æ•¸æ“šåº«ç²å–æœ€å¾ŒåŒæ­¥çš„æ™‚é–“æˆ³"""
        query = """
            SELECT MAX(timestamp) as last_timestamp
            FROM trades
        """
        result = self.db.execute(query).fetchone()
        
        if result and result['last_timestamp']:
            return int(result['last_timestamp'].timestamp())
        
        # å¦‚æœæ²’æœ‰æ•¸æ“šï¼Œå¾ 30 å¤©å‰é–‹å§‹
        return int((datetime.now() - timedelta(days=30)).timestamp())
    
    def save_last_synced_timestamp(self, timestamp):
        """ä¿å­˜æœ€å¾ŒåŒæ­¥çš„æ™‚é–“æˆ³åˆ° sync_state è¡¨"""
        query = """
            INSERT INTO sync_state (service_name, last_timestamp, updated_at)
            VALUES ('orderbook_collector', %s, NOW())
            ON DUPLICATE KEY UPDATE
                last_timestamp = VALUES(last_timestamp),
                updated_at = NOW()
        """
        self.db.execute(query, (timestamp,))
        self.db.commit()
    
    async def collect_events(self):
        """æ”¶é›†æ–°çš„ orderFilledEvents"""
        last_timestamp = self.get_last_synced_timestamp()
        logger.info(f"Starting collection from timestamp: {last_timestamp}")
        
        total_events = 0
        
        while True:
            # GraphQL æŸ¥è©¢
            query = gql("""
                query GetOrderFilledEvents($startTime: BigInt!, $limit: Int!) {
                    orderFilledEvents(
                        where: { timestamp_gt: $startTime }
                        orderBy: timestamp
                        orderDirection: asc
                        first: $limit
                    ) {
                        id
                        timestamp
                        maker
                        makerAssetId
                        makerAmountFilled
                        taker
                        takerAssetId
                        takerAmountFilled
                        transactionHash
                        fee
                    }
                }
            """)
            
            params = {
                "startTime": str(last_timestamp),
                "limit": self.batch_size
            }
            
            try:
                # åŸ·è¡ŒæŸ¥è©¢
                result = await self.execute_query(query, params)
                events = result.get('orderFilledEvents', [])
                
                if not events:
                    logger.info("No more events to collect")
                    break
                
                # è™•ç†äº‹ä»¶
                processed_count = await self.process_events(events)
                total_events += processed_count
                
                # æ›´æ–° last_timestamp
                last_timestamp = int(events[-1]['timestamp'])
                self.save_last_synced_timestamp(last_timestamp)
                
                logger.info(f"Processed {processed_count} events, total: {total_events}")
                
                # å¦‚æœè¿”å›çš„äº‹ä»¶å°‘æ–¼æ‰¹æ¬¡å¤§å°ï¼Œèªªæ˜å·²ç¶“åˆ°æœ€æ–°
                if len(events) < self.batch_size:
                    break
                
                # é¿å…è«‹æ±‚éå¿«
                await asyncio.sleep(0.5)
                
            except Exception as e:
                logger.error(f"Error collecting events: {e}")
                # é‡è©¦é‚è¼¯
                await asyncio.sleep(5)
                continue
        
        logger.info(f"âœ… Collection completed, total events: {total_events}")
        return total_events
```

### èª¿åº¦ç­–ç•¥

#### 1. å¯¦æ™‚æ›´æ–°ä»»å‹™
**é »ç‡**: æ¯ 5 åˆ†é˜  
**è§¸ç™¼**: Cron job

```bash
# crontab
*/5 * * * * cd /path/to/python-backend && python3 -m collectors.orderbook_collector
```

**æµç¨‹**:
1. ç²å–ä¸Šæ¬¡åŒæ­¥çš„æ™‚é–“æˆ³
2. æŸ¥è©¢æ–°çš„ orderFilledEvents
3. è™•ç†å’Œä¿å­˜äº‹ä»¶
4. æ›´æ–°åœ°å€çµ±è¨ˆ
5. è¨˜éŒ„åŒæ­¥ç‹€æ…‹

#### 2. éŒ¯èª¤æ¢å¾©æ©Ÿåˆ¶
- è‡ªå‹•é‡è©¦ï¼ˆæœ€å¤š 3 æ¬¡ï¼‰
- æŒ‡æ•¸é€€é¿ï¼ˆ1s, 2s, 4sï¼‰
- éŒ¯èª¤æ—¥èªŒè¨˜éŒ„
- å¤±æ•—é€šçŸ¥ï¼ˆç™¼é€è­¦å ±ï¼‰

```python
async def execute_query_with_retry(self, query, params, max_retries=3):
    """åŸ·è¡ŒæŸ¥è©¢ï¼Œå¸¶é‡è©¦æ©Ÿåˆ¶"""
    for attempt in range(max_retries):
        try:
            result = await self.client.execute(query, variable_values=params)
            return result
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # æŒ‡æ•¸é€€é¿
                logger.warning(f"Query failed (attempt {attempt + 1}/{max_retries}), retrying in {wait_time}s: {e}")
                await asyncio.sleep(wait_time)
            else:
                logger.error(f"Query failed after {max_retries} attempts: {e}")
                # ç™¼é€è­¦å ±é€šçŸ¥
                await self.send_alert(f"Orderbook collector failed: {e}")
                raise
```

---

## ğŸ” åœ°å€ç™¼ç¾å’Œè¿½è¹¤ç­–ç•¥

### è‡ªå‹•åœ°å€ç™¼ç¾

#### å¾äº¤æ˜“äº‹ä»¶æå–åœ°å€

```python
class AddressDiscovery:
    """åœ°å€ç™¼ç¾æœå‹™"""
    
    async def discover_from_events(self, events):
        """å¾ orderFilledEvents ç™¼ç¾æ–°åœ°å€"""
        discovered_addresses = set()
        
        for event in events:
            # æå– maker å’Œ taker åœ°å€
            maker = event['maker'].lower()
            taker = event['taker'].lower()
            
            discovered_addresses.add(maker)
            discovered_addresses.add(taker)
        
        # æ‰¹é‡æª¢æŸ¥å“ªäº›æ˜¯æ–°åœ°å€
        new_addresses = await self.filter_new_addresses(discovered_addresses)
        
        # æ‰¹é‡æ’å…¥æ–°åœ°å€
        if new_addresses:
            await self.insert_addresses(new_addresses)
            logger.info(f"Discovered {len(new_addresses)} new addresses")
        
        return new_addresses
    
    async def filter_new_addresses(self, addresses):
        """éæ¿¾å‡ºæ–°åœ°å€"""
        if not addresses:
            return []
        
        # æŸ¥è©¢æ•¸æ“šåº«ä¸­å·²å­˜åœ¨çš„åœ°å€
        placeholders = ','.join(['%s'] * len(addresses))
        query = f"""
            SELECT address FROM addresses
            WHERE address IN ({placeholders})
        """
        
        existing = set(row['address'] for row in self.db.execute(query, tuple(addresses)))
        
        # è¿”å›æ–°åœ°å€
        return addresses - existing
    
    async def insert_addresses(self, addresses):
        """æ‰¹é‡æ’å…¥æ–°åœ°å€"""
        if not addresses:
            return
        
        values = [(addr, datetime.now(), datetime.now()) for addr in addresses]
        
        query = """
            INSERT INTO addresses (address, first_seen_at, last_active_at)
            VALUES (%s, %s, %s)
        """
        
        self.db.executemany(query, values)
        self.db.commit()
```

### åœ°å€åˆ†é¡ç­–ç•¥

#### è‡ªå‹•æ¨™ç±¤

```python
class AddressLabeling:
    """åœ°å€æ¨™ç±¤æœå‹™"""
    
    # å·¨é¯¨é–¾å€¼
    WHALE_THRESHOLD = 10000  # $10,000
    
    # å¯ç–‘è©•åˆ†é–¾å€¼
    SUSPICIOUS_THRESHOLD = 70  # 70/100
    
    async def update_address_labels(self, address_id):
        """æ›´æ–°åœ°å€æ¨™ç±¤"""
        stats = await self.get_address_stats(address_id)
        
        labels = []
        
        # å·¨é¯¨æª¢æ¸¬
        if stats['total_volume'] >= self.WHALE_THRESHOLD:
            labels.append('whale')
        
        # é«˜é »äº¤æ˜“è€…
        if stats['total_trades'] > 100 and stats['avg_trade_interval'] < 3600:
            labels.append('high_frequency')
        
        # å¤§é¡äº¤æ˜“è€…
        if stats['avg_trade_size'] > 1000:
            labels.append('large_trader')
        
        # å¯ç–‘åœ°å€
        suspicion_score = await self.calculate_suspicion_score(address_id)
        if suspicion_score >= self.SUSPICIOUS_THRESHOLD:
            labels.append('suspicious')
        
        # æ›´æ–°æ•¸æ“šåº«
        await self.save_labels(address_id, labels)
        
        return labels
```

---

## ğŸ“¦ æ•¸æ“šè£œå…¨æ–¹æ¡ˆ

### æ­·å²æ•¸æ“šå›å¡«

#### ç­–ç•¥
1. **ç¢ºå®šå›å¡«ç¯„åœ**: éå» 30 å¤©çš„æ•¸æ“š
2. **åˆ†æ‰¹å›å¡«**: æ¯æ‰¹ 1000 å€‹äº‹ä»¶
3. **å„ªå…ˆç´š**: æ´»èºå¸‚å ´å„ªå…ˆ
4. **é€²åº¦è¿½è¹¤**: è¨˜éŒ„å›å¡«é€²åº¦

#### å¯¦ç¾

```python
class HistoricalDataBackfill:
    """æ­·å²æ•¸æ“šå›å¡«æœå‹™"""
    
    async def backfill(self, start_date, end_date):
        """å›å¡«æ­·å²æ•¸æ“š"""
        start_timestamp = int(start_date.timestamp())
        end_timestamp = int(end_date.timestamp())
        
        logger.info(f"Starting backfill from {start_date} to {end_date}")
        
        total_events = 0
        current_timestamp = start_timestamp
        
        while current_timestamp < end_timestamp:
            # æŸ¥è©¢ä¸€æ‰¹äº‹ä»¶
            events = await self.fetch_events(current_timestamp, self.batch_size)
            
            if not events:
                break
            
            # è™•ç†äº‹ä»¶
            processed = await self.process_events(events)
            total_events += processed
            
            # æ›´æ–°é€²åº¦
            current_timestamp = int(events[-1]['timestamp'])
            progress = (current_timestamp - start_timestamp) / (end_timestamp - start_timestamp) * 100
            
            logger.info(f"Backfill progress: {progress:.1f}%, total events: {total_events}")
            
            # ä¿å­˜é€²åº¦
            await self.save_progress(current_timestamp)
            
            # é¿å…è«‹æ±‚éå¿«
            await asyncio.sleep(1)
        
        logger.info(f"âœ… Backfill completed, total events: {total_events}")
        return total_events
    
    async def resume_backfill(self):
        """å¾ä¸Šæ¬¡ä¸­æ–·è™•æ¢å¾©å›å¡«"""
        last_timestamp = await self.get_last_backfill_timestamp()
        
        if last_timestamp:
            logger.info(f"Resuming backfill from timestamp: {last_timestamp}")
            start_date = datetime.fromtimestamp(last_timestamp)
        else:
            start_date = datetime.now() - timedelta(days=30)
        
        end_date = datetime.now()
        
        return await self.backfill(start_date, end_date)
```

### ç¼ºå¤±æ•¸æ“šæª¢æ¸¬

```python
class MissingDataDetector:
    """ç¼ºå¤±æ•¸æ“šæª¢æ¸¬å™¨"""
    
    async def detect_gaps(self):
        """æª¢æ¸¬æ™‚é–“åºåˆ—ä¸­çš„ç¼ºå£"""
        query = """
            SELECT 
                timestamp,
                LEAD(timestamp) OVER (ORDER BY timestamp) as next_timestamp
            FROM trades
            ORDER BY timestamp
        """
        
        results = self.db.execute(query).fetchall()
        
        gaps = []
        for row in results:
            if row['next_timestamp']:
                gap_seconds = (row['next_timestamp'] - row['timestamp']).total_seconds()
                
                # å¦‚æœé–“éš”è¶…é 1 å°æ™‚ï¼Œèªç‚ºæ˜¯ç¼ºå£
                if gap_seconds > 3600:
                    gaps.append({
                        'start': row['timestamp'],
                        'end': row['next_timestamp'],
                        'duration': gap_seconds
                    })
        
        if gaps:
            logger.warning(f"Found {len(gaps)} data gaps")
            for gap in gaps:
                logger.warning(f"Gap: {gap['start']} to {gap['end']} ({gap['duration']/3600:.1f} hours)")
        
        return gaps
    
    async def fill_gaps(self, gaps):
        """å¡«è£œç¼ºå£"""
        for gap in gaps:
            logger.info(f"Filling gap: {gap['start']} to {gap['end']}")
            await self.backfill(gap['start'], gap['end'])
```

---

## ğŸ—„ï¸ æ•¸æ“šå­˜å„²çµæ§‹å„ªåŒ–

### æ•¸æ“šåº« Schema æ“´å±•

#### 1. æ“´å±• trades è¡¨

```typescript
// drizzle/schema.ts

export const trades = mysqlTable("trades", {
  id: int("id").autoincrement().primaryKey(),
  
  // å¸‚å ´ä¿¡æ¯
  marketId: int("marketId").notNull(),
  conditionId: varchar("conditionId", { length: 255 }),
  
  // äº¤æ˜“åŸºæœ¬ä¿¡æ¯
  tradeId: varchar("tradeId", { length: 255 }).notNull().unique(),
  transactionHash: varchar("transactionHash", { length: 66 }).notNull(),
  
  // Maker ä¿¡æ¯
  makerAddress: varchar("makerAddress", { length: 42 }).notNull(),
  makerAssetId: varchar("makerAssetId", { length: 255 }).notNull(),
  makerAmount: int("makerAmount").notNull(), // åŸå§‹é‡‘é¡ï¼ˆä»¥æœ€å°å–®ä½ï¼‰
  
  // Taker ä¿¡æ¯
  takerAddress: varchar("takerAddress", { length: 42 }).notNull(),
  takerAssetId: varchar("takerAssetId", { length: 255 }).notNull(),
  takerAmount: int("takerAmount").notNull(),
  
  // äº¤æ˜“è©³æƒ…
  side: mysqlEnum("side", ["YES", "NO"]).notNull(),
  price: int("price").notNull(), // ä»¥åˆ†ç‚ºå–®ä½
  amount: int("amount").notNull(), // äº¤æ˜“é‡‘é¡ï¼ˆç¾å…ƒï¼Œä»¥åˆ†ç‚ºå–®ä½ï¼‰
  fee: int("fee").default(0), // æ‰‹çºŒè²»
  
  // æ¨™è¨˜
  isWhale: boolean("isWhale").default(false).notNull(),
  isSuspicious: boolean("isSuspicious").default(false).notNull(),
  
  // æ™‚é–“æˆ³
  timestamp: timestamp("timestamp").notNull(),
  createdAt: timestamp("createdAt").defaultNow().notNull(),
}, (table) => ({
  // ç´¢å¼•å„ªåŒ–
  makerAddressIdx: index("maker_address_idx").on(table.makerAddress),
  takerAddressIdx: index("taker_address_idx").on(table.takerAddress),
  timestampIdx: index("timestamp_idx").on(table.timestamp),
  transactionHashIdx: index("transaction_hash_idx").on(table.transactionHash),
}));
```

#### 2. å‰µå»º addresses è¡¨

```typescript
export const addresses = mysqlTable("addresses", {
  id: int("id").autoincrement().primaryKey(),
  
  // åœ°å€ä¿¡æ¯
  address: varchar("address", { length: 42 }).notNull().unique(),
  label: varchar("label", { length: 255 }), // ç”¨æˆ¶è‡ªå®šç¾©æ¨™ç±¤
  
  // çµ±è¨ˆæ•¸æ“š
  totalVolume: int("totalVolume").default(0), // ç¸½äº¤æ˜“é‡ï¼ˆç¾å…ƒï¼Œä»¥åˆ†ç‚ºå–®ä½ï¼‰
  totalTrades: int("totalTrades").default(0), // ç¸½äº¤æ˜“æ¬¡æ•¸
  avgTradeSize: int("avgTradeSize").default(0), // å¹³å‡äº¤æ˜“é‡‘é¡
  winRate: int("winRate").default(0), // å‹ç‡ï¼ˆ0-100ï¼‰
  realizedPnl: int("realizedPnl").default(0), // å·²å¯¦ç¾ç›ˆè™§
  
  // è¡Œç‚ºç‰¹å¾µ
  suspicionScore: int("suspicionScore").default(0), // å¯ç–‘è©•åˆ†ï¼ˆ0-100ï¼‰
  tradingFrequency: int("tradingFrequency").default(0), // äº¤æ˜“é »ç‡ï¼ˆæ¬¡/å¤©ï¼‰
  avgHoldingPeriod: int("avgHoldingPeriod").default(0), // å¹³å‡æŒå€‰æ™‚é–“ï¼ˆç§’ï¼‰
  
  // æ¨™è¨˜
  isWhale: boolean("isWhale").default(false).notNull(),
  isSuspicious: boolean("isSuspicious").default(false).notNull(),
  isHighFrequency: boolean("isHighFrequency").default(false).notNull(),
  
  // æ™‚é–“æˆ³
  firstSeenAt: timestamp("firstSeenAt").notNull(),
  lastActiveAt: timestamp("lastActiveAt").notNull(),
  createdAt: timestamp("createdAt").defaultNow().notNull(),
  updatedAt: timestamp("updatedAt").defaultNow().onUpdateNow().notNull(),
}, (table) => ({
  addressIdx: index("address_idx").on(table.address),
  isWhaleIdx: index("is_whale_idx").on(table.isWhale),
  isSuspiciousIdx: index("is_suspicious_idx").on(table.isSuspicious),
}));
```

#### 3. å‰µå»º sync_state è¡¨

```typescript
export const syncState = mysqlTable("sync_state", {
  id: int("id").autoincrement().primaryKey(),
  
  // æœå‹™åç¨±
  serviceName: varchar("serviceName", { length: 100 }).notNull().unique(),
  
  // åŒæ­¥ç‹€æ…‹
  lastTimestamp: int("lastTimestamp").notNull(), // æœ€å¾ŒåŒæ­¥çš„æ™‚é–“æˆ³
  lastSyncAt: timestamp("lastSyncAt").notNull(), // æœ€å¾ŒåŒæ­¥æ™‚é–“
  status: mysqlEnum("status", ["idle", "running", "error"]).default("idle").notNull(),
  errorMessage: text("errorMessage"),
  
  // çµ±è¨ˆ
  totalProcessed: int("totalProcessed").default(0), // ç¸½è™•ç†æ•¸é‡
  lastBatchSize: int("lastBatchSize").default(0), // æœ€å¾Œä¸€æ‰¹çš„å¤§å°
  
  createdAt: timestamp("createdAt").defaultNow().notNull(),
  updatedAt: timestamp("updatedAt").defaultNow().onUpdateNow().notNull(),
});
```

### æ•¸æ“šåº«é·ç§»è…³æœ¬

```bash
# é‹è¡Œé·ç§»
pnpm db:push

# é©—è­‰ schema
pnpm db:studio
```

---

## ğŸ›¡ï¸ API é™æµå’ŒéŒ¯èª¤è™•ç†

### API é™æµç­–ç•¥

#### Goldsky API é™åˆ¶
- **é€Ÿç‡é™åˆ¶**: æœªå…¬é–‹ï¼ˆä¼°è¨ˆ ~100 req/minï¼‰
- **æ‰¹æ¬¡å¤§å°**: å»ºè­° 1000 å€‹äº‹ä»¶/æ¬¡
- **ä¸¦ç™¼è«‹æ±‚**: å»ºè­° 1 å€‹ï¼ˆé¿å…è§¸ç™¼é™åˆ¶ï¼‰

#### å¯¦ç¾

```python
class RateLimiter:
    """API é€Ÿç‡é™åˆ¶å™¨"""
    
    def __init__(self, max_requests=60, time_window=60):
        self.max_requests = max_requests
        self.time_window = time_window  # ç§’
        self.requests = []
    
    async def acquire(self):
        """ç²å–è«‹æ±‚è¨±å¯"""
        now = time.time()
        
        # æ¸…ç†éæœŸçš„è«‹æ±‚è¨˜éŒ„
        self.requests = [req_time for req_time in self.requests 
                        if now - req_time < self.time_window]
        
        # æª¢æŸ¥æ˜¯å¦è¶…éé™åˆ¶
        if len(self.requests) >= self.max_requests:
            # è¨ˆç®—éœ€è¦ç­‰å¾…çš„æ™‚é–“
            oldest_request = min(self.requests)
            wait_time = self.time_window - (now - oldest_request)
            
            if wait_time > 0:
                logger.warning(f"Rate limit reached, waiting {wait_time:.1f}s")
                await asyncio.sleep(wait_time)
        
        # è¨˜éŒ„æœ¬æ¬¡è«‹æ±‚
        self.requests.append(time.time())
    
    async def __aenter__(self):
        await self.acquire()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        pass

# ä½¿ç”¨ç¤ºä¾‹
rate_limiter = RateLimiter(max_requests=60, time_window=60)

async def fetch_data():
    async with rate_limiter:
        # åŸ·è¡Œ API è«‹æ±‚
        result = await api_client.query(...)
        return result
```

### éŒ¯èª¤è™•ç†ç­–ç•¥

#### éŒ¯èª¤åˆ†é¡

1. **ç¶²çµ¡éŒ¯èª¤** (Network Errors)
   - é€£æ¥è¶…æ™‚
   - DNS è§£æå¤±æ•—
   - é€£æ¥é‡ç½®
   - **è™•ç†**: è‡ªå‹•é‡è©¦ï¼ˆæœ€å¤š 3 æ¬¡ï¼‰

2. **API éŒ¯èª¤** (API Errors)
   - é€Ÿç‡é™åˆ¶ï¼ˆ429ï¼‰
   - æœå‹™å™¨éŒ¯èª¤ï¼ˆ500, 502, 503ï¼‰
   - **è™•ç†**: æŒ‡æ•¸é€€é¿é‡è©¦

3. **æ•¸æ“šéŒ¯èª¤** (Data Errors)
   - ç¼ºå¤±æ¬„ä½
   - æ•¸æ“šæ ¼å¼éŒ¯èª¤
   - é‡è¤‡æ•¸æ“š
   - **è™•ç†**: è¨˜éŒ„æ—¥èªŒï¼Œè·³éè©²æ¢æ•¸æ“š

4. **æ•¸æ“šåº«éŒ¯èª¤** (Database Errors)
   - é€£æ¥å¤±æ•—
   - é–è¶…æ™‚
   - ç´„æŸé•å
   - **è™•ç†**: å›æ»¾äº‹å‹™ï¼Œé‡è©¦

#### å¯¦ç¾

```python
class ErrorHandler:
    """éŒ¯èª¤è™•ç†å™¨"""
    
    @staticmethod
    async def handle_api_error(error, context):
        """è™•ç† API éŒ¯èª¤"""
        if isinstance(error, aiohttp.ClientError):
            # ç¶²çµ¡éŒ¯èª¤
            logger.error(f"Network error in {context}: {error}")
            return 'retry'
        
        elif hasattr(error, 'status'):
            if error.status == 429:
                # é€Ÿç‡é™åˆ¶
                logger.warning(f"Rate limit hit in {context}")
                return 'backoff'
            elif error.status >= 500:
                # æœå‹™å™¨éŒ¯èª¤
                logger.error(f"Server error in {context}: {error.status}")
                return 'retry'
            else:
                # å…¶ä»– HTTP éŒ¯èª¤
                logger.error(f"HTTP error in {context}: {error.status}")
                return 'skip'
        
        else:
            # æœªçŸ¥éŒ¯èª¤
            logger.error(f"Unknown error in {context}: {error}")
            return 'skip'
    
    @staticmethod
    async def handle_data_error(error, data, context):
        """è™•ç†æ•¸æ“šéŒ¯èª¤"""
        logger.error(f"Data error in {context}: {error}")
        logger.debug(f"Problematic data: {data}")
        
        # è¨˜éŒ„åˆ°éŒ¯èª¤æ—¥èªŒ
        await ErrorHandler.log_error({
            'context': context,
            'error': str(error),
            'data': data,
            'timestamp': datetime.now()
        })
        
        return 'skip'
    
    @staticmethod
    async def log_error(error_info):
        """è¨˜éŒ„éŒ¯èª¤åˆ°æ•¸æ“šåº«"""
        # å¯ä»¥å‰µå»ºä¸€å€‹ error_logs è¡¨
        pass

# ä½¿ç”¨ç¤ºä¾‹
async def process_event(event):
    try:
        # è™•ç†äº‹ä»¶
        result = await process(event)
        return result
    except Exception as e:
        action = await ErrorHandler.handle_data_error(e, event, 'process_event')
        
        if action == 'skip':
            return None
        elif action == 'retry':
            # é‡è©¦é‚è¼¯
            return await process_event(event)
```

### ç›£æ§å’Œè­¦å ±

#### ç›£æ§æŒ‡æ¨™

1. **æ•¸æ“šæ”¶é›†æŒ‡æ¨™**
   - æ¯åˆ†é˜è™•ç†çš„äº‹ä»¶æ•¸
   - API è«‹æ±‚æˆåŠŸç‡
   - å¹³å‡éŸ¿æ‡‰æ™‚é–“
   - éŒ¯èª¤ç‡

2. **æ•¸æ“šè³ªé‡æŒ‡æ¨™**
   - æ•¸æ“šå®Œæ•´æ€§ï¼ˆç¼ºå¤±æ¬„ä½æ¯”ä¾‹ï¼‰
   - æ•¸æ“šæ–°é®®åº¦ï¼ˆæœ€æ–°æ•¸æ“šçš„æ™‚é–“æˆ³ï¼‰
   - æ•¸æ“šç¼ºå£æ•¸é‡

3. **ç³»çµ±å¥åº·æŒ‡æ¨™**
   - æœå‹™é‹è¡Œæ™‚é–“
   - å…§å­˜ä½¿ç”¨é‡
   - æ•¸æ“šåº«é€£æ¥æ•¸
   - éšŠåˆ—é•·åº¦

#### è­¦å ±è¦å‰‡

```python
class AlertManager:
    """è­¦å ±ç®¡ç†å™¨"""
    
    async def check_and_alert(self):
        """æª¢æŸ¥æŒ‡æ¨™ä¸¦ç™¼é€è­¦å ±"""
        # æª¢æŸ¥æ•¸æ“šæ–°é®®åº¦
        latest_trade = await self.get_latest_trade_timestamp()
        data_age = datetime.now() - latest_trade
        
        if data_age.total_seconds() > 600:  # 10 åˆ†é˜
            await self.send_alert(
                title="æ•¸æ“šæ›´æ–°å»¶é²",
                message=f"æœ€æ–°æ•¸æ“šå·²ç¶“ {data_age.total_seconds()/60:.1f} åˆ†é˜æœªæ›´æ–°",
                severity="warning"
            )
        
        # æª¢æŸ¥éŒ¯èª¤ç‡
        error_rate = await self.get_error_rate()
        
        if error_rate > 0.1:  # 10%
            await self.send_alert(
                title="éŒ¯èª¤ç‡éé«˜",
                message=f"ç•¶å‰éŒ¯èª¤ç‡: {error_rate*100:.1f}%",
                severity="error"
            )
        
        # æª¢æŸ¥æ•¸æ“šç¼ºå£
        gaps = await self.detect_data_gaps()
        
        if gaps:
            await self.send_alert(
                title="ç™¼ç¾æ•¸æ“šç¼ºå£",
                message=f"ç™¼ç¾ {len(gaps)} å€‹æ•¸æ“šç¼ºå£",
                severity="warning"
            )
    
    async def send_alert(self, title, message, severity):
        """ç™¼é€è­¦å ±é€šçŸ¥"""
        # å¯ä»¥é€šéå¤šç¨®æ–¹å¼ç™¼é€ï¼š
        # 1. ç³»çµ±é€šçŸ¥è¡¨
        # 2. éƒµä»¶
        # 3. Slack/Discord webhook
        # 4. SMS
        
        logger.log(
            logging.ERROR if severity == "error" else logging.WARNING,
            f"[{severity.upper()}] {title}: {message}"
        )
        
        # ä¿å­˜åˆ°æ•¸æ“šåº«
        await self.save_notification(title, message, severity)
```

---

## ğŸ“ˆ æ•ˆèƒ½å„ªåŒ–

### æ‰¹é‡è™•ç†

```python
class BatchProcessor:
    """æ‰¹é‡è™•ç†å™¨"""
    
    def __init__(self, batch_size=1000):
        self.batch_size = batch_size
    
    async def process_in_batches(self, items, processor_func):
        """æ‰¹é‡è™•ç†é …ç›®"""
        total_processed = 0
        
        for i in range(0, len(items), self.batch_size):
            batch = items[i:i + self.batch_size]
            
            try:
                # è™•ç†æ‰¹æ¬¡
                result = await processor_func(batch)
                total_processed += len(batch)
                
                logger.info(f"Processed batch {i//self.batch_size + 1}, total: {total_processed}/{len(items)}")
                
            except Exception as e:
                logger.error(f"Error processing batch: {e}")
                # å¯ä»¥é¸æ“‡è·³éæˆ–é‡è©¦
        
        return total_processed
```

### æ•¸æ“šåº«å„ªåŒ–

#### 1. æ‰¹é‡æ’å…¥

```python
async def bulk_insert_trades(trades):
    """æ‰¹é‡æ’å…¥äº¤æ˜“è¨˜éŒ„"""
    if not trades:
        return
    
    # æº–å‚™æ‰¹é‡æ’å…¥çš„æ•¸æ“š
    values = [
        (
            trade['marketId'],
            trade['tradeId'],
            trade['makerAddress'],
            trade['takerAddress'],
            # ... å…¶ä»–æ¬„ä½
        )
        for trade in trades
    ]
    
    query = """
        INSERT INTO trades (
            marketId, tradeId, makerAddress, takerAddress, ...
        ) VALUES (%s, %s, %s, %s, ...)
        ON DUPLICATE KEY UPDATE
            updatedAt = NOW()
    """
    
    # ä½¿ç”¨ executemany æ‰¹é‡æ’å…¥
    cursor.executemany(query, values)
    conn.commit()
    
    logger.info(f"Bulk inserted {len(trades)} trades")
```

#### 2. ç´¢å¼•å„ªåŒ–

```sql
-- ç‚ºå¸¸ç”¨æŸ¥è©¢æ·»åŠ ç´¢å¼•
CREATE INDEX idx_trades_timestamp ON trades(timestamp);
CREATE INDEX idx_trades_maker ON trades(makerAddress);
CREATE INDEX idx_trades_taker ON trades(takerAddress);
CREATE INDEX idx_addresses_whale ON addresses(isWhale);
CREATE INDEX idx_addresses_suspicious ON addresses(isSuspicious);

-- è¤‡åˆç´¢å¼•
CREATE INDEX idx_trades_market_timestamp ON trades(marketId, timestamp);
```

#### 3. æŸ¥è©¢å„ªåŒ–

```python
# âŒ ä¸å¥½çš„æŸ¥è©¢ï¼ˆN+1 å•é¡Œï¼‰
for address in addresses:
    trades = db.query(f"SELECT * FROM trades WHERE makerAddress = '{address}'")
    # è™•ç† trades

# âœ… å¥½çš„æŸ¥è©¢ï¼ˆæ‰¹é‡æŸ¥è©¢ï¼‰
addresses_str = ','.join([f"'{addr}'" for addr in addresses])
trades = db.query(f"SELECT * FROM trades WHERE makerAddress IN ({addresses_str})")
# åˆ†çµ„è™•ç† trades
```

---

## ğŸš€ éƒ¨ç½²å’Œé‹ç¶­

### éƒ¨ç½²æ¶æ§‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  æ•¸æ“šæ”¶é›†æœå‹™ (Python)                             â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  â€¢ Docker å®¹å™¨                                     â”‚  â”‚
â”‚  â”‚  â€¢ è‡ªå‹•é‡å•Ÿï¼ˆsystemd/supervisorï¼‰                  â”‚  â”‚
â”‚  â”‚  â€¢ æ—¥èªŒæ”¶é›†ï¼ˆstdout â†’ file/syslogï¼‰                â”‚  â”‚
â”‚  â”‚  â€¢ å¥åº·æª¢æŸ¥ï¼ˆæ¯ 5 åˆ†é˜ï¼‰                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â†“                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  èª¿åº¦æœå‹™ (Cron)                                   â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  â€¢ å¯¦æ™‚æ›´æ–°: */5 * * * *                           â”‚  â”‚
â”‚  â”‚  â€¢ å¸‚å ´æ›´æ–°: 0 * * * *                             â”‚  â”‚
â”‚  â”‚  â€¢ çµ±è¨ˆæ›´æ–°: 0 0 * * *                             â”‚  â”‚
â”‚  â”‚  â€¢ å¥åº·æª¢æŸ¥: */1 * * * *                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â†“                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  æ•¸æ“šåº« (MySQL)                                    â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  â€¢ ä¸»å¾è¤‡è£½ï¼ˆè®€å¯«åˆ†é›¢ï¼‰                             â”‚  â”‚
â”‚  â”‚  â€¢ å®šæœŸå‚™ä»½ï¼ˆæ¯å¤©ï¼‰                                 â”‚  â”‚
â”‚  â”‚  â€¢ æ…¢æŸ¥è©¢æ—¥èªŒ                                       â”‚  â”‚
â”‚  â”‚  â€¢ é€£æ¥æ± ç®¡ç†                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### é‹ç¶­æª¢æŸ¥æ¸…å–®

#### æ¯æ—¥æª¢æŸ¥
- [ ] æ•¸æ“šæ›´æ–°æ˜¯å¦æ­£å¸¸
- [ ] éŒ¯èª¤æ—¥èªŒæ˜¯å¦æœ‰ç•°å¸¸
- [ ] æ•¸æ“šåº«æ€§èƒ½æŒ‡æ¨™
- [ ] ç£ç›¤ç©ºé–“ä½¿ç”¨ç‡

#### æ¯é€±æª¢æŸ¥
- [ ] æ•¸æ“šå®Œæ•´æ€§é©—è­‰
- [ ] æ€§èƒ½æŒ‡æ¨™è¶¨å‹¢
- [ ] å‚™ä»½æ¢å¾©æ¸¬è©¦
- [ ] ä¾è³´æ›´æ–°æª¢æŸ¥

#### æ¯æœˆæª¢æŸ¥
- [ ] æ•¸æ“šåº«å„ªåŒ–ï¼ˆANALYZE TABLEï¼‰
- [ ] æ¸…ç†éæœŸæ•¸æ“š
- [ ] å®‰å…¨æ›´æ–°
- [ ] å®¹é‡è¦åŠƒ

---

## ğŸ“Š é æœŸæ•ˆæœ

### æ•¸æ“šæ™‚æ•ˆæ€§
- **ç•¶å‰**: æ•¸å°æ™‚å»¶é²ï¼ˆæ‰‹å‹•æ›´æ–°ï¼‰
- **ç›®æ¨™**: 5 åˆ†é˜å»¶é²ï¼ˆè‡ªå‹•æ›´æ–°ï¼‰
- **æå‡**: **95%+**

### åœ°å€è¦†è“‹ç‡
- **ç•¶å‰**: æ•¸ç™¾å€‹åœ°å€
- **ç›®æ¨™**: æ•¸è¬å€‹åœ°å€
- **æå‡**: **100å€+**

### æ•¸æ“šå®Œæ•´æ€§
- **ç•¶å‰**: 0 ç­†äº¤æ˜“è¨˜éŒ„
- **ç›®æ¨™**: å®Œæ•´çš„äº¤æ˜“æ­·å²
- **æå‡**: **ç„¡é™**

### ç³»çµ±å¯é æ€§
- **ç•¶å‰**: æ‰‹å‹•é‹è¡Œï¼Œä¸ç©©å®š
- **ç›®æ¨™**: 99.9% æ­£å¸¸é‹è¡Œæ™‚é–“
- **æå‡**: **é¡¯è‘—**

---

## ğŸ¯ ä¸‹ä¸€æ­¥è¡Œå‹•

### Phase 4: å¯¦ä½œåœ°å€è¿½è¹¤å’Œåˆ†æåŠŸèƒ½
1. [ ] å¯¦ç¾ OrderbookCollector
2. [ ] å¯¦ç¾ AddressDiscovery
3. [ ] å¯¦ç¾ EventProcessor
4. [ ] æ“´å±•æ•¸æ“šåº« schema
5. [ ] å¯¦ç¾æ‰¹é‡è™•ç†é‚è¼¯

### Phase 5: å¯¦ä½œå¯ç–‘åœ°å€åµæ¸¬ç³»çµ±
1. [ ] å®šç¾©å¯ç–‘è¡Œç‚ºç‰¹å¾µ
2. [ ] å¯¦ç¾ç•°å¸¸æª¢æ¸¬ç®—æ³•
3. [ ] å¯¦ç¾é¢¨éšªè©•åˆ†ç³»çµ±
4. [ ] å¯¦ç¾è­¦å ±é€šçŸ¥

### Phase 6: æ¸¬è©¦å’Œå„ªåŒ–
1. [ ] å–®å…ƒæ¸¬è©¦
2. [ ] é›†æˆæ¸¬è©¦
3. [ ] æ€§èƒ½æ¸¬è©¦
4. [ ] éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ

---

## ğŸ“š åƒè€ƒè³‡æ–™

### æŠ€è¡“æ–‡æª”
- Goldsky Subgraph API: https://docs.goldsky.com/
- Drizzle ORM: https://orm.drizzle.team/
- GraphQL: https://graphql.org/

### åƒè€ƒé …ç›®
- warproxxx/poly_data: https://github.com/warproxxx/poly_data
- polymarket-apis: https://pypi.org/project/polymarket-apis/

### æœ€ä½³å¯¦è¸
- å¢é‡æ•¸æ“šåŒæ­¥è¨­è¨ˆæ¨¡å¼
- éŒ¯èª¤è™•ç†å’Œé‡è©¦ç­–ç•¥
- æ•¸æ“šåº«ç´¢å¼•å„ªåŒ–
- API é€Ÿç‡é™åˆ¶è™•ç†
